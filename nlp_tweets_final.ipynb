{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment of the tweets\n",
    "\n",
    "### Input\n",
    "amazon_cells_labelled.txt\n",
    "\n",
    "imdb_labelled.txt\n",
    "\n",
    "yelp_labelled.txt\n",
    "\n",
    "path for input=/home/wk/Machine Learning - WKordas/tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wk/Machine Learning - WKordas\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "def CleanEndChars (line:str)->str:\n",
    "    line = re.sub(r\"\\r\", '',line)\n",
    "    line = re.sub(r\"\\t\", '',line)\n",
    "    line = re.sub(r\"\\n\", '',line)   \n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CleanText (text:str)->str:\n",
    "    text = text.replace('$', ' dollar ')\n",
    "    text = text.replace('+', ' plus ')\n",
    "    text = text.replace('%', ' percent ')\n",
    "    text = text.replace('<', ' lower than ')\n",
    "    text = text.replace('>', ' greater than ')  \n",
    "    text = text.replace(' 1 ', ' one ')\n",
    "    text = text.replace(' 2 ', ' two ')\n",
    "    text = text.replace(' 3 ', ' three ')\n",
    "    text = text.replace(' 4 ', ' four ')\n",
    "    text = text.replace(' 5 ', ' five ')\n",
    "    text = text.replace(\"'s\", ' ')\n",
    "    text = re.sub(r'[^\\'\\w\\s]',' ',text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English words with apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnglishWordsWithApostrophe = {\n",
    "r\"ain't\": \"am not\",\n",
    "r\"aren't\": \"are not\",\n",
    "r\"can't\": \"cannot\",\n",
    "r\"can't've\": \"cannot have\",\n",
    "r\"'cause\": \"because\",\n",
    "r\"could've\": \"could have\",\n",
    "r\"couldn't\": \"could not\",\n",
    "r\"couldn't've\": \"could not have\",\n",
    "r\"didn't\": \"did not\",\n",
    "r\"doesn't\": \"does not\",\n",
    "r\"don't\": \"do not\",\n",
    "r\"hadn't\": \"had not\",\n",
    "r\"hadn't've\": \"had not have\",\n",
    "r\"hasn't\": \"has not\",\n",
    "r\"haven't\": \"have not\",\n",
    "r\"he'd\": \"he would\",\n",
    "r\"he'd've\": \"he would have\",\n",
    "r\"he'll\": \"he will\",\n",
    "r\"he'll've\": \"he will have\",\n",
    "r\"he's\": \"he is\",\n",
    "r\"how'd\": \"how did\",\n",
    "r\"how'd'y\": \"how do you\",\n",
    "r\"how'll\": \"how will\",\n",
    "r\"how's\": \"how is\",\n",
    "r\"i'd\": \"i would\",\n",
    "r\"i'd've\": \"i would have\",\n",
    "r\"i'll\": \"i will\",\n",
    "r\"i'll've\": \"i will have\",\n",
    "r\"i'm\": \"i am\",\n",
    "r\"i've\": \"i have\",\n",
    "r\"isn't\": \"is not\",\n",
    "r\"it'd\": \"it would\",\n",
    "r\"it'd've\": \"it would have\",\n",
    "r\"it'll\": \"it will\",\n",
    "r\"it'll've\": \"it will have\",\n",
    "r\"it's\": \"it is\",\n",
    "r\"let's\": \"let us\",\n",
    "r\"ma'am\": \"madam\",\n",
    "r\"mayn't\": \"may not\",\n",
    "r\"might've\": \"might have\",\n",
    "r\"mightn't\": \"might not\",\n",
    "r\"mightn't've\": \"might not have\",\n",
    "r\"must've\": \"must have\",\n",
    "r\"mustn't\": \"must not\",\n",
    "r\"mustn't've\": \"must not have\",\n",
    "r\"needn't\": \"need not\",\n",
    "r\"needn't've\": \"need not have\",\n",
    "r\"o'clock\": \"of the clock\",\n",
    "r\"oughtn't\": \"ought not\",\n",
    "r\"oughtn't've\": \"ought not have\",\n",
    "r\"shan't\": \"shall not\",\n",
    "r\"sha'n't\": \"shall not\",\n",
    "r\"shan't've\": \"shall not have\",\n",
    "r\"she'd\": \"she would\",\n",
    "r\"she'd've\": \"she would have\",\n",
    "r\"she'll\": \"she will\",\n",
    "r\"she'll've\": \"she will have\",\n",
    "r\"she's\": \"she is\",\n",
    "r\"should've\": \"should have\",\n",
    "r\"shouldn't\": \"should not\",\n",
    "r\"shouldn't've\": \"should not have\",\n",
    "r\"so've\": \"so have\",\n",
    "r\"so's\": \"so as\",\n",
    "r\"that'd\": \"that would\",\n",
    "r\"that'd've\": \"that would have\",\n",
    "r\"that's\": \"that is\",\n",
    "r\"there'd\": \"there would\",\n",
    "r\"there'd've\": \"there would have\",\n",
    "r\"there's\": \"there is\",\n",
    "r\"they'd\": \"they would\",\n",
    "r\"they'd've\": \"they would have\",\n",
    "r\"they'll\": \"they will\",\n",
    "r\"they'll've\": \"they will have\",\n",
    "r\"they're\": \"they are\",\n",
    "r\"they've\": \"they have\",\n",
    "r\"to've\": \"to have\",\n",
    "r\"wasn't\": \"was not\",\n",
    "r\"we'd\": \"we would\",\n",
    "r\"we'd've\": \"we would have\",\n",
    "r\"we'll\": \"we will\",\n",
    "r\"we'll've\": \"we will have\",\n",
    "r\"we're\": \"we are\",\n",
    "r\"we've\": \"we have\",\n",
    "r\"weren't\": \"were not\",\n",
    "r\"what'll\": \"what will\",\n",
    "r\"what'll've\": \"what will have\",\n",
    "r\"what're\": \"what are\",\n",
    "r\"what's\": \"what is\",\n",
    "r\"what've\": \"what have\",\n",
    "r\"when's\": \"when is\",\n",
    "r\"when've\": \"when have\",\n",
    "r\"where'd\": \"where did\",\n",
    "r\"where's\": \"where is\",\n",
    "r\"where've\": \"where have\",\n",
    "r\"who'll\": \"who will\",\n",
    "r\"who'll've\": \"who will have\",\n",
    "r\"who's\": \"who is\",\n",
    "r\"who've\": \"who have\",\n",
    "r\"why's\": \"why is\",\n",
    "r\"why've\": \"why have\",\n",
    "r\"will've\": \"will have\",\n",
    "r\"won't\": \"will not\",\n",
    "r\"won't've\": \"will not have\",\n",
    "r\"would've\": \"would have\",\n",
    "r\"wouldn't\": \"would not\",\n",
    "r\"wouldn't've\": \"would not have\",\n",
    "r\"y'all\": \"you all\",\n",
    "r\"y'all'd\": \"you all would\",\n",
    "r\"y'all'd've\": \"you all would have\",\n",
    "r\"y'all're\": \"you all are\",\n",
    "r\"y'all've\": \"you all have\",\n",
    "r\"you'd\": \"you would\",\n",
    "r\"you'd've\": \"you would have\",\n",
    "r\"you'll\": \"you will\",\n",
    "r\"you'll've\": \"you will have\",\n",
    "r\"you're\": \"you are\",\n",
    "r\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with English Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnglishAbbreviations = {\n",
    "r\"121\": \"one to one\",\n",
    "r\"a/s/l\": \"age sex location\",\n",
    "r\"adn\": \"any day now\",\n",
    "r\"afaik\": \"as far as I know\",\n",
    "r\"afk\": \"away from keyboard\",\n",
    "r\"aight\": \"alright\",\n",
    "r\"alol\": \"actually laughing out loud\",\n",
    "r\"b4\": \"before\",\n",
    "r\"b4n\": \"bye for now\",\n",
    "r\"bak\": \"back at the keyboard\",\n",
    "r\"bf\": \"boyfriend\",\n",
    "r\"bff\": \"best friends forever\",\n",
    "r\"bfn\": \"bye for now\",\n",
    "r\"bg\": \"big grin\",\n",
    "r\"bta\": \"but then again\",\n",
    "r\"btw\": \"by the way\",\n",
    "r\"cid\": \"crying in disgrace\",\n",
    "r\"cnp\": \"continued in my next post\",\n",
    "r\"cp\": \"chat post\",\n",
    "r\"cu\": \"see you\",\n",
    "r\"cul\": \"see you later\",\n",
    "r\"cul8r\": \"see you later\",\n",
    "r\"cya\": \"bye\",\n",
    "r\"cyo\": \"see you online\",\n",
    "r\"dbau\": \"doing business as usual\",\n",
    "r\"fud\": \"fear uncertainty and doubt\",\n",
    "r\"fwiw\": \"for what it is worth\",\n",
    "r\"fyi\": \"for your information\",\n",
    "r\"g\": \"grin\",\n",
    "r\"g2g\": \"got to go\",\n",
    "r\"ga\": \"go ahead\",\n",
    "r\"gal\": \"get a life\",\n",
    "r\"gf\": \"girlfriend\",\n",
    "r\"gfn\": \"gone for now\",\n",
    "r\"gmbo\": \"giggling my butt off\",\n",
    "r\"gmta\": \"great minds think alike\",\n",
    "r\"gr8\": \"great\",   \n",
    "r\"h8\": \"hate\",\n",
    "r\"hagn\": \"have a good night\",\n",
    "r\"hdop\": \"help delete online predators\",\n",
    "r\"hhis\": \"hanging head in shame\",\n",
    "r\"iac\": \"in any case\",\n",
    "r\"ianal\": \"I am not a lawyer\",\n",
    "r\"ic\": \"I see\",\n",
    "r\"idk\": \"I don't know\",\n",
    "r\"imao\": \"in my arrogant opinion\",\n",
    "r\"imnsho\": \"in my not so humble opinion\",\n",
    "r\"imo\": \"in my opinion\",\n",
    "r\"iow\": \"in other words\",\n",
    "r\"ipn\": \"I am posting naked\",\n",
    "r\"irl\": \"in real life\",\n",
    "r\"jk\": \"just kidding\",\n",
    "r\"l8r\": \"later\",\n",
    "r\"ld\": \"later, dude\",\n",
    "r\"ldr\": \"long distance relationship\",\n",
    "r\"llta\": \"lots and lots of thunderous applause\",\n",
    "r\"lmao\": \"laugh my ass off\",\n",
    "r\"lmirl\": \"let's meet in real life\",\n",
    "r\"lol\": \"laugh out loud\",\n",
    "r\"ltr\": \"longterm relationship\",\n",
    "r\"lulab\": \"love you like a brother\",\n",
    "r\"lulas\": \"love you like a sister\",\n",
    "r\"luv\": \"love\",\n",
    "r\"m/f\": \"male or female\",\n",
    "r\"m8\": \"mate\",\n",
    "r\"milf\": \"mother I would like to fuck\",\n",
    "r\"oll\": \"online love\",\n",
    "r\"omg\": \"oh my god\",\n",
    "r\"otoh\": \"on the other hand\",\n",
    "r\"pir\": \"parent in room\",\n",
    "r\"ppl\": \"people\",\n",
    "r\"r\": \"are\",\n",
    "r\"rofl\": \"roll on the floor laughing\",\n",
    "r\"rpg\": \"role playing games\",\n",
    "r\"ru\": \"are you\",\n",
    "r\"shid\": \"slaps head in disgust\",\n",
    "r\"somy\": \"sick of me yet\",\n",
    "r\"sot\": \"short of time\",\n",
    "r\"thanx\": \"thanks\",\n",
    "r\"thx\": \"thanks\",\n",
    "r\"ttyl\": \"talk to you later\",\n",
    "r\"u\": \"you\",\n",
    "r\"ur\": \"you are\",\n",
    "r\"uw\": \"you are welcome\",\n",
    "r\"the us\":\"united states\",    \n",
    "r\"wb\": \"welcome back\",\n",
    "r\"wfm\": \"works for me\",\n",
    "r\"wibni\": \"would not it be nice if\",\n",
    "r\"wtf\": \"what the fuck\",\n",
    "r\"wtg\": \"way to go\",\n",
    "r\"wtgp\": \"want to go private\",\n",
    "r\"ym\": \"young man\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with e-moticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons = {\n",
    "r':\\)': 'happy',\n",
    "r':â€‘\\)': 'happy',\n",
    "r':-\\]': 'happy',\n",
    "r':-3': 'happy',\n",
    "r':->': 'happy',\n",
    "r'8-\\)': 'happy',\n",
    "r':-\\}': 'happy',\n",
    "r':o\\)': 'happy',\n",
    "r':c\\)': 'happy',\n",
    "r':\\^\\)': 'happy',\n",
    "r'=\\]': 'happy',\n",
    "r'=\\)': 'happy',\n",
    "r'<3': 'happy',\n",
    "r':-\\(': 'unhappy',\n",
    "r':\\(': 'unhappy',\n",
    "r':c': 'unhappy',\n",
    "r':<': 'unhappy',\n",
    "r':\\[': 'unhappy',\n",
    "r'>:\\[': 'unhappy',\n",
    "r':\\{': 'unhappy',\n",
    "r'>:\\(': 'unhappy',\n",
    "r':-c': 'unhappy',\n",
    "r':-< ': 'unhappy',\n",
    "r':-\\[': 'unhappy',\n",
    "r':-\\|\\|': 'unhappy'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing words with apostrophes,abbreviations and e-moticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordsWithApostrophe (txt:str)->str:     \n",
    "    for el in EnglishWordsWithApostrophe.keys():\n",
    "        txt = re.sub(el,' ' + EnglishWordsWithApostrophe[el] + ' ',txt)\n",
    "    return (txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def CleanApostrophesEmoticonsAbbr (txt:str)->str:\n",
    "    \n",
    "    txt = txt.lower()   \n",
    "# words with apostrophe   \n",
    "    txt = WordsWithApostrophe(txt) \n",
    "\n",
    "        \n",
    "# Emoticons        \n",
    "    for el in emoticons.keys(): \n",
    "        txt = re.sub(el,' ' + emoticons[el] + ' ',txt)  \n",
    "\n",
    "\n",
    "# Abbreviations    \n",
    "    data = txt.split()\n",
    "    for el in EnglishAbbreviations.keys():\n",
    "        for i, x in enumerate(data):\n",
    "            if x == el:\n",
    "                data[i] = EnglishAbbreviations[el]\n",
    "    txt = ' '.join(data)\n",
    "        \n",
    "      \n",
    "    return txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning list from not key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not important words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "WordsToRemove =set(stopwords.words('english'))\n",
    "\n",
    "def CleanList (data:list())->list():\n",
    "    for word in WordsToRemove:\n",
    "        for i,el in enumerate (data):\n",
    "            if el == word:\n",
    "                data.pop(i)\n",
    "            elif len(el)<=1:\n",
    "                data.pop(i)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean line (removing non letters, muliplications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def CleanLine (line:str)->str:\n",
    "\n",
    "    # removing non letters\n",
    "    line = re.sub(r'[^a-zA-Z]',' ',line[:-1])+line[-1:]\n",
    "    \n",
    "    # removing mulpltication of the letters\n",
    "    for char in string.ascii_lowercase:\n",
    "        line = re.sub(char +'{3,}', char+char, line) \n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# errors corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import enchant\n",
    "from enchant.checker import SpellChecker\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "def CorrectErr (word:str, dic:str='en_GB')->str:\n",
    "    sdict = enchant.Dict(dic) #GB dictionary\n",
    "    proper_words = sdict.suggest(word)\n",
    "    if len(proper_words) == 0:\n",
    "        return ' '\n",
    "    dproper_words = dict()\n",
    "\n",
    "    for proper_word in proper_words:\n",
    "        if edit_distance(word, proper_word) == 0:\n",
    "            return word\n",
    "        else:\n",
    "            dproper_words[proper_word] = edit_distance(word, proper_word)\n",
    "    sorted_dproper_words = sorted(dproper_words.items(),key = lambda el: el[1])  \n",
    "    len_s = 0\n",
    "    best_el = sorted_dproper_words[0][1]\n",
    "    for kel,vel in sorted_dproper_words:\n",
    "        len_s += 1\n",
    "        if vel>best_el:\n",
    "            break\n",
    "    rpos = random.randint(0,len_s-1)\n",
    "    word = sorted_dproper_words[rpos][0]\n",
    "   \n",
    "    return word\n",
    "\n",
    "\n",
    "def CorrectErrInList (l:list())->list():\n",
    "    for i,el in enumerate(l):\n",
    "        l[i] =CorrectErr(el)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standarising words to a basic form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing library for lemmatizing\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizing = WordNetLemmatizer()\n",
    "\n",
    "def BasicForm (word:str)->str:\n",
    "    word = lemmatizing.lemmatize(word,'a')\n",
    "    word = lemmatizing.lemmatize(word,'n')\n",
    "    word = lemmatizing.lemmatize(word,'v')\n",
    "    return word\n",
    "\n",
    "def ListBasicForm (l:list())->str:\n",
    "    for i,el in enumerate (l):\n",
    "        l[i] = BasicForm(el)\n",
    "    return ' '.join(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading input files and putting data into data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import nltk\n",
    "# emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veryfing existance of other type of emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import emoji\n",
    "\n",
    "#emoji_dict = dict()\n",
    "#emoji.EMOJI_UNICODE\n",
    "#for key in emoji.EMOJI_UNICODE:\n",
    "#    emoji_dict [emoji.EMOJI_UNICODE[key]] = key\n",
    "\n",
    "#def FindEmoji (line:str)->list:\n",
    "#    le=list()\n",
    "#    for key in emoji_dict:\n",
    "#        if key in line:           \n",
    "#            le.add (key)\n",
    "#    l= regexp_tokenize(line, emoji)\n",
    "#    if len(l) > 0:\n",
    "#        print (line,' ===',l)\n",
    "#    return le\n",
    "#\n",
    "#dftweets['tweet'].apply(lambda line:  FindEmoji (line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "grinning face \tU+1F600\n",
    "grinning face with big eyes \tU+1F603\n",
    "grinning face with smiling eyes \tU+1F604\n",
    "beaming face with smiling eyes \tU+1F601\n",
    "grinning squinting face \tU+1F606\n",
    "grinning face with sweat \tU+1F605\n",
    "rolling on the floor laughing \tU+1F923\n",
    "face with tears of joy \tU+1F602\n",
    "slightly smiling face \tU+1F642\n",
    "upside-down face \tU+1F643\n",
    "winking face \tU+1F609\n",
    "smiling face with smiling eyes \tU+1F60A\n",
    "smiling face with halo \tU+1F607\n",
    "smiling face with 3 hearts \tU+1F970\n",
    "smiling face with heart-eyes \tU+1F60D\n",
    "star-struck \tU+1F929\n",
    "face blowing a kiss \tU+1F618\n",
    "kissing face \tU+1F617\n",
    "smiling face \tU+263A\n",
    "kissing face with closed eyes \tU+1F61A\n",
    "kissing face with smiling eyes \tU+1F619\n",
    "face savoring food \tU+1F60B\n",
    "face with tongue \tU+1F61B\n",
    "winking face with tongue \tU+1F61C\n",
    "zany face \tU+1F92A\n",
    "squinting face with tongue \tU+1F61D\n",
    "money-mouth face \tU+1F911\n",
    "hugging face \tU+1F917\n",
    "face with hand over mouth \tU+1F92D\n",
    "shushing face \tU+1F92B\n",
    "thinking face \tU+1F914\n",
    "zipper-mouth face \tU+1F910\n",
    "face with raised eyebrow \tU+1F928\n",
    "neutral face \tU+1F610\n",
    "expressionless face \tU+1F611\n",
    "face without mouth \tU+1F636\n",
    "smirking face \tU+1F60F\n",
    "unamused face \tU+1F612\n",
    "face with rolling eyes \tU+1F644\n",
    "grimacing face \tU+1F62C\n",
    "lying face \tU+1F925\n",
    "relieved face \tU+1F60C\n",
    "pensive face \tU+1F614\n",
    "sleepy face \tU+1F62A\n",
    "drooling face \tU+1F924\n",
    "sleeping face \tU+1F634\n",
    "face with medical mask \tU+1F637\n",
    "face with thermometer \tU+1F912\n",
    "face with head-bandage \tU+1F915\n",
    "nauseated face \tU+1F922\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 10 columns):\n",
      "ident        3000 non-null int64\n",
      "tweet        3000 non-null object\n",
      "sentiment    3000 non-null int64\n",
      "tweet1       3000 non-null object\n",
      "tweet2       3000 non-null object\n",
      "tweet3       3000 non-null object\n",
      "tweet4       3000 non-null object\n",
      "tweet5       3000 non-null object\n",
      "tweet6       3000 non-null object\n",
      "tweet7       3000 non-null object\n",
      "dtypes: int64(2), object(8)\n",
      "memory usage: 234.5+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets = list()        \n",
    "sentiments = list()\n",
    "\n",
    "#Reading tweets from amazon file\n",
    "f=open(r'./tweets/amazon_cells_labelled.txt','r',encoding = 'utf-8')\n",
    "for line in f:\n",
    "    line = CleanEndChars(line)\n",
    "    tweets.append (line[:-1])\n",
    "    sentiments.append (int(line[-1:][0]))  \n",
    "f.close()\n",
    "\n",
    "#Reading tweets from imdb file\n",
    "f=open(r'./tweets/imdb_labelled.txt','r',encoding = 'utf-8')\n",
    "for line in f:\n",
    "    line = CleanEndChars(line)\n",
    "    tweets.append (line[:-1])\n",
    "    sentiments.append (int(line[-1:][0]))  \n",
    "f.close()\n",
    "\n",
    "#Reading tweets from yelp file\n",
    "f=open(r'./tweets/yelp_labelled.txt','r',encoding = 'utf-8')\n",
    "for line in f:\n",
    "    line = CleanEndChars(line)\n",
    "    tweets.append (line[:-1])\n",
    "    sentiments.append (int(line[-1:][0]))  \n",
    "f.close()\n",
    "\n",
    "# creating data frame\n",
    "dtweets = {'ident':[i for i in range(1,len(sentiments)+1)],'tweet':tweets,'sentiment':sentiments}\n",
    "dftweets = pd.DataFrame(dtweets)\n",
    "\n",
    "\n",
    "dftweets['tweet1'] =dftweets['tweet'].apply(lambda line:  CleanApostrophesEmoticonsAbbr (line))\n",
    "dftweets['tweet2'] =dftweets['tweet1'].apply(lambda line:  CleanText (line))\n",
    "dftweets['tweet3'] =dftweets['tweet2'].apply(lambda line:  CleanLine (line))\n",
    "dftweets['tweet4'] =dftweets['tweet3'].apply(lambda line:  CleanList (line.split()))\n",
    "dftweets['tweet5'] =dftweets['tweet4'].apply(lambda line:  CorrectErrInList (line))\n",
    "dftweets['tweet6'] =dftweets['tweet5'].apply(lambda line:  WordsWithApostrophe (' '.join(line)).split())\n",
    "dftweets['tweet7'] =dftweets['tweet6'].apply(lambda line:  ListBasicForm (line))\n",
    "\n",
    " \n",
    "dftweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final data set to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftweets.to_excel('./tweets/tweets.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "Tfidf_results = dict()\n",
    "Count_results = dict()\n",
    "results = {'Tfidf':Tfidf_results,'Count':Count_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer\n",
      "Logistic regression:\n",
      "accuracy train=  0.8858333333333334  accuracy test =  0.8016666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA:\n",
      "accuracy train=  0.9208333333333333  accuracy test =  0.7416666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA:\n",
      "accuracy train=  0.835  accuracy test =  0.6766666666666666\n",
      "Neighbors:\n",
      "accuracy train=  0.6445833333333333  accuracy test =  0.6316666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Votting hard:\n",
      "accuracy train=  0.91375  accuracy test =  0.7533333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Votting soft:\n",
      "accuracy train=  0.90875  accuracy test =  0.7333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging with LDA:\n",
      "accuracy train=  0.9308333333333333  accuracy test =  0.7616666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging with QDA:\n",
      "accuracy train=  0.9204166666666667  accuracy test =  0.6766666666666666\n",
      "AdaBoost:\n",
      "accuracy train=  0.7466666666666667  accuracy test =  0.725\n",
      "AdaBoost based RandomForest:\n",
      "accuracy train=  0.9804166666666667  accuracy test =  0.7533333333333333\n",
      "XGBoost:\n",
      "accuracy train=  0.7508333333333334  accuracy test =  0.725\n",
      "Fitting 4 folds for each of 36 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.9min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tfidf = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "tfidf_array = tfidf.fit_transform(dftweets['tweet7'])\n",
    "\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(tfidf_array, dftweets['sentiment'], random_state=42, test_size=0.2)\n",
    "X_train = xtrain.toarray()\n",
    "y_train = ytrain.values\n",
    "X_test = xtest.toarray()\n",
    "y_test = ytest.values\n",
    "\n",
    "print ('TfidfVectorizer')\n",
    "# Logistic Regression\n",
    "\n",
    "logreg1 = LogisticRegression()\n",
    "logreg1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg1.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(ytrain,logreg1.predict(xtrain))\n",
    "accuracy_test = accuracy_score(ytest,logreg1.predict(xtest))\n",
    "Tfidf_results ['Logistic regression'] = [accuracy_train, accuracy_test]\n",
    "print ('Logistic regression:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# LDA\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit (X_train,y_train)\n",
    "y_pred = lda.predict (X_test)\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,lda.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,lda.predict(X_test))\n",
    "Tfidf_results ['LDA'] = [accuracy_train, accuracy_test]\n",
    "print ('LDA:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# QDA \n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit (X_train,y_train)\n",
    "y_pred = qda.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,qda.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,qda.predict(X_test))\n",
    "Tfidf_results ['QDA'] = [accuracy_train, accuracy_test]\n",
    "print ('QDA:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "               \n",
    "# Neighbors\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn= KNeighborsClassifier(n_neighbors=11)\n",
    "knn.fit (X_train,y_train)\n",
    "y_pred = knn.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,knn.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,knn.predict(X_test))\n",
    "Tfidf_results ['Neighbors'] = [accuracy_train, accuracy_test]\n",
    "print ('Neighbors:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# Voting hard\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "clf1 = LinearDiscriminantAnalysis()\n",
    "clf2= QuadraticDiscriminantAnalysis()\n",
    "clf3= KNeighborsClassifier()\n",
    "\n",
    "elcf1 = VotingClassifier(estimators = [('lda',clf1),('qda',clf2),('knn',clf3)],voting = 'hard') # hard - glosowanie wiekszosciowe\n",
    "elcf1.fit (X_train,y_train)\n",
    "    \n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,elcf1.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,elcf1.predict(X_test))\n",
    "Tfidf_results ['Votting hard'] = [accuracy_train, accuracy_test]\n",
    "print ('Votting hard:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# Voting soft\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf1 = LinearDiscriminantAnalysis()\n",
    "clf2= QuadraticDiscriminantAnalysis()\n",
    "clf3= KNeighborsClassifier()\n",
    "\n",
    "elcf2 = VotingClassifier(estimators = [('lda',clf1),('qda',clf2),('knn',clf3)],voting = 'soft') \n",
    "elcf2.fit (X_train,y_train)\n",
    "y_pred = elcf2.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,elcf2.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,elcf2.predict(X_test))\n",
    "Tfidf_results ['Votting soft'] = [accuracy_train, accuracy_test]\n",
    "print ('Votting soft:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# Bagging with LDA\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "bagging = BaggingClassifier (LinearDiscriminantAnalysis(),random_state=1)\n",
    "bagging.fit (X_train,y_train)\n",
    "y_pred = bagging.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,bagging.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,bagging.predict(X_test))\n",
    "Tfidf_results ['Bagging with LDA'] = [accuracy_train, accuracy_test]\n",
    "print ('Bagging with LDA:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# Bagging with QDA\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "bagging = BaggingClassifier (QuadraticDiscriminantAnalysis(),random_state=1)\n",
    "bagging.fit (X_train,y_train)\n",
    "y_pred = bagging.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,bagging.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,bagging.predict(X_test))\n",
    "Tfidf_results ['Bagging with QDA'] = [accuracy_train, accuracy_test]\n",
    "print ('Bagging with QDA:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# AdaBoost\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "clf = AdaBoostClassifier() # Bez parametru default - drzewo decyzyjne\n",
    "clf.fit (X_train,y_train)\n",
    "y_pred = clf.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,clf.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,clf.predict(X_test))\n",
    "Tfidf_results ['AdaBoost'] = [accuracy_train, accuracy_test]\n",
    "print ('AdaBoost:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# AdaBoost\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator = RandomForestClassifier(n_estimators=20)) # wybieramy las losowy (X_train,y_train)\n",
    "clf.fit (X_train,y_train)\n",
    "y_pred = clf.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,clf.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,clf.predict(X_test))\n",
    "Tfidf_results ['Adaboost based RandomForest'] = [accuracy_train, accuracy_test]\n",
    "print ('AdaBoost based RandomForest:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# XGBOOST\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "xg_cl = xgb.XGBClassifier()\n",
    "xg_cl.fit (X_train,y_train)\n",
    "y_pred = xg_cl.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,xg_cl.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,xg_cl.predict(X_test))\n",
    "Tfidf_results ['XGBoost'] = [accuracy_train, accuracy_test]\n",
    "print ('XGBoost:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# GridSearch\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'reg_lambda':(0.1,1,10),'reg_alpha':(0.01,0.1,1,10),'max_depth':(7,9,11)}\n",
    "xg_grid = xgb.XGBClassifier(random_state = 1)\n",
    "xg_grid_cv = GridSearchCV(estimator = xg_grid,param_grid = params,scoring = 'accuracy',cv=4,verbose=1,n_jobs=-1,refit = True)\n",
    "xg_grid_cv.fit (X_train,y_train)\n",
    "print ('Tfidfvectorizer, GridSearch')\n",
    "print ('Best parameters found=',xg_grid_cv.best_params_)\n",
    "print ('Best Accuracy=',xg_grid_cv.best_score_)\n",
    "\n",
    "#xg_grid_cv.fit (X_train,y_train)\n",
    "y_pred = xg_grid_cv.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,xg_grid_cv.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,xg_grid_cv.predict(X_test))\n",
    "Tfidf_results ['Xgboost - GridSearch'] = [accuracy_train, accuracy_test]\n",
    "print ('Xgboost - GridSearch:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "logreg = LogisticRegression()\n",
    "pipeline = make_pipeline (scaler,pca,logreg)\n",
    "\n",
    "pipeline.fit (X_train,y_train)\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,pipeline.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,pipeline.predict(X_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(ytrain,pipeline.predict(X_train))\n",
    "accuracy_test = accuracy_score(ytest,pipeline.predict(X_test))\n",
    "Tfidf_results ['PCA'] = [accuracy_train, accuracy_test]\n",
    "print ('PCA:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer\n",
      "Logistic regression:\n",
      "accuracy train=  0.8979166666666667  accuracy test =  0.8033333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA:\n",
      "accuracy train=  0.9145833333333333  accuracy test =  0.7383333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA:\n",
      "accuracy train=  0.9275  accuracy test =  0.7083333333333334\n",
      "Neighbors:\n",
      "accuracy train=  0.7408333333333333  accuracy test =  0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Hard:\n",
      "accuracy train=  0.9383333333333334  accuracy test =  0.7666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Votting soft:\n",
      "accuracy train=  0.9379166666666666  accuracy test =  0.755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging with LDA:\n",
      "accuracy train=  0.9308333333333333  accuracy test =  0.7566666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:692: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging with QDA:\n",
      "accuracy train=  0.9283333333333333  accuracy test =  0.7233333333333334\n",
      "AdaBoost:\n",
      "accuracy train=  0.7491666666666666  accuracy test =  0.7566666666666667\n",
      "Adaboost based RandomForest:\n",
      "accuracy train=  0.98125  accuracy test =  0.7383333333333333\n",
      "XGBoost:\n",
      "accuracy train=  0.7491666666666666  accuracy test =  0.7433333333333333\n",
      "Fitting 4 folds for each of 36 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed: 15.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countvectorizer, GridSearch\n",
      "Best parameters found= {'max_depth': 9, 'reg_alpha': 0.01, 'reg_lambda': 0.1}\n",
      "Best Accuracy= 0.7516666666666667\n",
      "XGBoost GridSearch:\n",
      "accuracy train=  0.8283333333333334  accuracy test =  0.7866666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA:\n",
      "accuracy train=  0.94875  accuracy test =  0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wk/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "countv = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "countv_array = countv.fit_transform(dftweets['tweet7'])\n",
    "\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(countv_array, dftweets['sentiment'], random_state=42, test_size=0.2)\n",
    "X_train = xtrain.toarray()\n",
    "y_train = ytrain.values\n",
    "X_test = xtest.toarray()\n",
    "y_test = ytest.values\n",
    "print ('CountVectorizer')\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "logreg1 = LogisticRegression()\n",
    "logreg1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg1.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(ytrain,logreg1.predict(xtrain))\n",
    "accuracy_test = accuracy_score(ytest,logreg1.predict(xtest))\n",
    "Count_results ['Logistic regression'] = [accuracy_train, accuracy_test]\n",
    "print ('Logistic regression:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# LDA\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit (X_train,y_train)\n",
    "y_pred = lda.predict (X_test)\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,lda.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,lda.predict(X_test))\n",
    "Count_results ['LDA'] = [accuracy_train, accuracy_test]\n",
    "print ('LDA:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# QDA \n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit (X_train,y_train)\n",
    "y_pred = qda.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,qda.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,qda.predict(X_test))\n",
    "Count_results ['QDA'] = [accuracy_train, accuracy_test]\n",
    "print ('QDA:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "               \n",
    "# Neighbors\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn= KNeighborsClassifier(n_neighbors=11)\n",
    "knn.fit (X_train,y_train)\n",
    "y_pred = knn.predict (X_test)\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,knn.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,knn.predict(X_test))\n",
    "Count_results ['Neighbors'] = [accuracy_train, accuracy_test]\n",
    "print ('Neighbors:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# Voting hard\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "clf1 = LinearDiscriminantAnalysis()\n",
    "clf2= QuadraticDiscriminantAnalysis()\n",
    "clf3= KNeighborsClassifier()\n",
    "\n",
    "elcf1 = VotingClassifier(estimators = [('lda',clf1),('qda',clf2),('knn',clf3)],voting = 'hard') # hard - glosowanie wiekszosciowe\n",
    "elcf1.fit (X_train,y_train)\n",
    "    \n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,elcf1.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,elcf1.predict(X_test))\n",
    "print ('Voting Hard:')\n",
    "Count_results ['Voting hard'] = [accuracy_train, accuracy_test]\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# Voting soft\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf1 = LinearDiscriminantAnalysis()\n",
    "clf2= QuadraticDiscriminantAnalysis()\n",
    "clf3= KNeighborsClassifier()\n",
    "\n",
    "elcf2 = VotingClassifier(estimators = [('lda',clf1),('qda',clf2),('knn',clf3)],voting = 'soft') \n",
    "elcf2.fit (X_train,y_train)\n",
    "y_pred = elcf2.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,elcf2.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,elcf2.predict(X_test))\n",
    "Count_results ['Votting soft'] = [accuracy_train, accuracy_test]\n",
    "print ('Votting soft:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# Bagging with LDA\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "bagging = BaggingClassifier (LinearDiscriminantAnalysis(),random_state=1)\n",
    "bagging.fit (X_train,y_train)\n",
    "y_pred = bagging.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,bagging.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,bagging.predict(X_test))\n",
    "Count_results ['Bagging with LDA'] = [accuracy_train, accuracy_test]\n",
    "print ('Bagging with LDA:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# Bagging with QDA\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "bagging = BaggingClassifier (QuadraticDiscriminantAnalysis(),random_state=1)\n",
    "bagging.fit (X_train,y_train)\n",
    "y_pred = bagging.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,bagging.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,bagging.predict(X_test))\n",
    "Count_results ['Bagging with QDA'] = [accuracy_train, accuracy_test]\n",
    "print ('Bagging with QDA:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# AdaBoost\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "clf = AdaBoostClassifier() # Bez parametru default - drzewo decyzyjne\n",
    "clf.fit (X_train,y_train)\n",
    "y_pred = clf.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,clf.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,clf.predict(X_test))\n",
    "Count_results ['AdaBoost'] = [accuracy_train, accuracy_test]\n",
    "print ('AdaBoost:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# AdaBoost\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator = RandomForestClassifier(n_estimators=20)) # wybieramy las losowy (X_train,y_train)\n",
    "clf.fit (X_train,y_train)\n",
    "y_pred = clf.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,clf.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,clf.predict(X_test))\n",
    "Count_results ['Adaboost based RandomForest'] = [accuracy_train, accuracy_test]\n",
    "print ('Adaboost based RandomForest:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# XGBOOST\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "xg_cl = xgb.XGBClassifier()\n",
    "xg_cl.fit (X_train,y_train)\n",
    "y_pred = xg_cl.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,xg_cl.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,xg_cl.predict(X_test))\n",
    "Count_results ['XGBoost'] = [accuracy_train, accuracy_test]\n",
    "print ('XGBoost:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "# GridSearch\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'reg_lambda':(0.1,1,10),'reg_alpha':(0.01,0.1,1,10),'max_depth':(7,9,11)}\n",
    "xg_grid = xgb.XGBClassifier(random_state = 1)\n",
    "xg_grid_cv = GridSearchCV(estimator = xg_grid,param_grid = params,scoring = 'accuracy',cv=4,verbose=1,n_jobs=-1,refit = True)\n",
    "xg_grid_cv.fit (X_train,y_train)\n",
    "print ('Countvectorizer, GridSearch')\n",
    "print ('Best parameters found=',xg_grid_cv.best_params_)\n",
    "print ('Best Accuracy=',xg_grid_cv.best_score_)\n",
    "\n",
    "#xg_grid_cv.fit (X_train,y_train)\n",
    "y_pred = xg_grid_cv.predict (X_test)\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,xg_grid_cv.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,xg_grid_cv.predict(X_test))\n",
    "print ('XGBoost GridSearch:')\n",
    "Count_results ['Xgboost - GridSearch'] = [accuracy_train, accuracy_test]\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "logreg = LogisticRegression()\n",
    "pipeline = make_pipeline (scaler,pca,logreg)\n",
    "\n",
    "pipeline.fit (X_train,y_train)\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(y_train,pipeline.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test,pipeline.predict(X_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sprawdzenie dokladnosci (accuracy)\n",
    "accuracy_train = accuracy_score(ytrain,pipeline.predict(X_train))\n",
    "accuracy_test = accuracy_score(ytest,pipeline.predict(X_test))\n",
    "Count_results ['PCA'] = [accuracy_train, accuracy_test]\n",
    "print ('PCA:')\n",
    "print ('accuracy train= ', accuracy_train,' accuracy test = ', accuracy_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('{:>45}{:>16}'.format('Tfidfvectorizer','Countvectorizer'))\n",
    "print ('{:>35}{:>6}{:>10}{:>6}'.format('train','test','train','test'))\n",
    "for key,el in results['Tfidf'].items():\n",
    "    print (key,el)\n",
    "    print ('{:28}  {:1.3f}  {:1.3f}    {:1.3f}  {:1.3f}'.format(\n",
    "           key, el[0],el[1],results['Count'][key][0],results['Count'][key][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tfidf': {'Logistic regression': [0.8858333333333334, 0.8016666666666666],\n",
       "  'LDA': [0.9208333333333333, 0.7416666666666667],\n",
       "  'QDA': [0.835, 0.6766666666666666],\n",
       "  'Neighbors': [0.6445833333333333, 0.6316666666666667],\n",
       "  'Votting hard': [0.91375, 0.7533333333333333],\n",
       "  'Votting soft': [0.90875, 0.7333333333333333],\n",
       "  'Bagging with LDA': [0.9308333333333333, 0.7616666666666667],\n",
       "  'Bagging with QDA': [0.9204166666666667, 0.6766666666666666],\n",
       "  'AdaBoost': [0.7466666666666667, 0.725],\n",
       "  'Adaboost based RandomForest': [0.98, 0.7516666666666667],\n",
       "  'XGBoost': [0.7508333333333334, 0.725],\n",
       "  'Xgboost - GridSearch': [0.8329166666666666, 0.7666666666666667],\n",
       "  'PCA': [0.9475, 0.715]},\n",
       " 'Count': {'Logistic regression': [0.8979166666666667, 0.8033333333333333],\n",
       "  'LDA': [0.9145833333333333, 0.7383333333333333],\n",
       "  'QDA': [0.9275, 0.7083333333333334],\n",
       "  'Neighbors': [0.7408333333333333, 0.66],\n",
       "  'Voting hard': [0.9383333333333334, 0.7666666666666667],\n",
       "  'Votting soft': [0.9379166666666666, 0.755],\n",
       "  'Bagging with LDA': [0.9308333333333333, 0.7566666666666667],\n",
       "  'Bagging with QDA': [0.9283333333333333, 0.7233333333333334],\n",
       "  'AdaBoost': [0.7491666666666666, 0.7566666666666667],\n",
       "  'Adaboost based RandomForest': [0.98125, 0.7383333333333333],\n",
       "  'XGBoost': [0.7491666666666666, 0.7433333333333333],\n",
       "  'Xgboost - GridSearch': [0.8283333333333334, 0.7866666666666666],\n",
       "  'PCA': [0.94875, 0.74]}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
